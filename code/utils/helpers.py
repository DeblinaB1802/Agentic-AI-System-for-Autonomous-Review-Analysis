import json
import re
import nltk
import pandas as pd
import numpy as np
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from datetime import datetime

nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('nltk')

def estimate_response_confidence(response: str, execution_time: float):
    """Estimate the confidence score of a model's response based on key characteristics.
        This function assumes a base confidence level and adjusts it based on:
        - Response length (longer responses generally indicate better content)
        - Presence of valid JSON structure
        - Response execution time (faster responses may suggest higher confidence)

        Parameters:
            response (str): The raw output generated by the model.
            execution_time (float): Time taken by the model to generate the response (in seconds).

        Returns:
            float: A confidence score between 0.0 and 1.0 indicating the estimated reliability of the response."""
    
    confidence = 0.5  
    if len(response) > 100:
        confidence += 0.1
    if len(response) > 500:
        confidence += 0.1
        
    try:
        json_matches = re.findall(r"\{.*?\}", response, flags=re.DOTALL)
        if json_matches:
            json.loads(json_matches[0])
            confidence += 0.2
    except:
        confidence -= 0.2
        
    if execution_time < 10:
        confidence += 0.1
    elif execution_time > 30:
        confidence -= 0.1
        
    return max(0.0, min(1.0, confidence))

def compute_word_diversity(df: pd.DataFrame):

    stemmer = PorterStemmer()
    all_reviews = " ".join(df['customer_review'].dropna().astype(str).apply(lambda x: x.lower()).tolist())
    tokens = word_tokenize(all_reviews)
    stopwords = set(stopwords.words('english'))
    filtered_tokens = [stemmer.stem(token) for token in tokens if token.isalpha() and token not in stopwords]
    
    vocab_richness = len(set(filtered_tokens)/len(filtered_tokens))
    return vocab_richness

def assess_data_quality(df: pd.DataFrame):
    quality_metrics = {}

    try:
        avg_review_len = df['customer_review'].astype(str).len().mean()
        quality_metrics['avg_review_length'] = min(avg_review_len/100, 1)

        diversity = len(df['customer_review'].unique())/len(df)
        quality_metrics['review_diversity'] = diversity

        date_range = df['review_date'].max() - df['review_date'].min()
        quality_metrics['temporal_spread'] = min(date_range.days/365, 1)

        quality_metrics['vocab_richness'] = compute_word_diversity(df)

        verified_purchase = df[df['verified_purchase'] == True]
        quality_metrics['verified_purchase_ratio'] = len(verified_purchase)/len(df)

        quality_metrics['rating_mean'] = df['rating'].mean()
        quality_metrics['rating_std'] = df['rating'].std()

        completeness = df.isnull().sum().sum()/(len(df) * len(df.columuns))
        quality_metrics['review_completeness'] = 1 - completeness

        quality_metrics['overall_score'] = np.mean(quality_metrics.values)
        return quality_metrics
    
    except Exception as e:
        print(f"Failed to analyze data quality: {str(e)}")
        return {}

def autonomous_task_prioritization(df: pd.DataFrame):
    priorities = []
    quality_params = assess_data_quality(df)

    priorities.append("sentiment")
    if quality_params['overall_score'] >= 0.5 and quality_params['avg_review_length'] > 0.7:
        priorities.append("summary")
    if quality_params['overall_score'] >= 0.6 and quality_params['verified_purchase_ratio'] > 0.5:
        priorities.append("issues")
    if len(df) > 100 and quality_params['temporal_spread'] >= 0.1:
        priorities.append("trend_analysis")
    if quality_params['diversity'] > 0.75 and quality_params['vocab_richness'] > 0.7:
        priorities.append("usp")
    if len(df) > 300 and quality_params['rating_std'] > 1.2:
        priorities.append("anomaly_detection")

    return priorities

def calculate_days_passed(date_str: str):

    past_date = datetime.strptime(date_str, "%Y-%m-%d")
    today = datetime.today()
    days_passed = (today - past_date).days

    return days_passed